{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "344cbeaf-c863-407d-a866-de2f5d4f1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc, concordance_index_censored, integrated_brier_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.util import Surv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c98d3-d054-4e0d-b178-70d243c520f5",
   "metadata": {},
   "source": [
    "<p>All 5 of the imputed datasets</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e584394-729e-4129-9b0b-75b142a9fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"../data/imputations/final_w_imps_1.csv\")\n",
    "train2 = pd.read_csv(\"../data/imputations/final_w_imps_2.csv\")\n",
    "train3 = pd.read_csv(\"../data/imputations/final_w_imps_3.csv\")\n",
    "train4 = pd.read_csv(\"../data/imputations/final_w_imps_4.csv\")\n",
    "train5 = pd.read_csv(\"../data/imputations/final_w_imps_5.csv\")\n",
    "\n",
    "train = [train1, train2, train3, train4, train5]\n",
    "\n",
    "test1 = pd.read_csv(\"../data/imputations/final_w_imps_1_test.csv\")\n",
    "test2 = pd.read_csv(\"../data/imputations/final_w_imps_2_test.csv\")\n",
    "test3 = pd.read_csv(\"../data/imputations/final_w_imps_3_test.csv\")\n",
    "test4 = pd.read_csv(\"../data/imputations/final_w_imps_4_test.csv\")\n",
    "test5 = pd.read_csv(\"../data/imputations/final_w_imps_5_test.csv\")\n",
    "\n",
    "test = [test1, test2, test3, test4, test5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f7122-e687-4e26-87bb-5dac033ae65c",
   "metadata": {},
   "source": [
    "<p>Removing the Pseudo-observation columns</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3659b841-dda9-46ed-a5d6-a6fa7f7f6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train)):\n",
    "    train[idx] = train[idx].loc[:, ~train[idx].columns.str.startswith(\"PO_\")]\n",
    "\n",
    "for idx in range(len(test)):\n",
    "    test[idx] = test[idx].loc[:, ~test[idx].columns.str.startswith(\"PO_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c66977-bf68-4eb6-b060-d6c6ac48200e",
   "metadata": {},
   "source": [
    "<p>Grouping ethnicities</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6532d056-46e6-4997-8b0b-2bbbd37b6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qrJos\\AppData\\Local\\Temp\\ipykernel_2248\\3172019743.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[i]['ethnicity'] = train[i]['ethnicity'].apply(collapse_ethnicity)\n",
      "C:\\Users\\qrJos\\AppData\\Local\\Temp\\ipykernel_2248\\3172019743.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[i]['ethnicity'] = test[i]['ethnicity'].apply(collapse_ethnicity)\n"
     ]
    }
   ],
   "source": [
    "def collapse_ethnicity(val):\n",
    "    val = str(val).upper()\n",
    "    if \"ASIAN\" in val:\n",
    "        return \"Asian\"\n",
    "    elif \"WHITE\" in val or \"MIDDLE\" in val:\n",
    "        return \"White\"\n",
    "    elif \"BLACK\" in val:\n",
    "        return \"Black or African American\"\n",
    "    elif any(word in val for word in [\"OTHER\", \"PATIENT\", \"UNABLE\", \"UNKNOWN\"]):\n",
    "        return \"Unknown\"\n",
    "    elif \"AMERICAN\" in val:\n",
    "        return \"American Indian or Alaska Native\"\n",
    "    elif \"HISPANIC\" in val:\n",
    "        return \"Hispanic\"\n",
    "    elif \"MULTI\" in val:\n",
    "        return \"More than one race\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "# Apply ethnicity collapsing to each dataframe in ds\n",
    "for i in range(len(train)):\n",
    "    if 'ethnicity' in train[i].columns:\n",
    "        train[i]['ethnicity'] = train[i]['ethnicity'].apply(collapse_ethnicity)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if 'ethnicity' in test[i].columns:\n",
    "        test[i]['ethnicity'] = test[i]['ethnicity'].apply(collapse_ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd6ae5-4a96-4075-838c-c567a202b54f",
   "metadata": {},
   "source": [
    "<p>Splitting into X and y for each dataset</p>\n",
    "<p>Combines into one dataset for onehot encoding before resplitting</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "386a041a-8ee3-4db9-9caa-b2c5c360f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitXY(train_dfs, test_dfs):\n",
    "    # Track lengths for re-splitting\n",
    "    train_lengths = [len(df) for df in train_dfs]\n",
    "    test_lengths = [len(df) for df in test_dfs]\n",
    "\n",
    "    # Combine all into one DataFrame (with origin label)\n",
    "    combined = pd.concat(train_dfs + test_dfs, ignore_index=True)\n",
    "\n",
    "    # Clip negative times\n",
    "    combined[\"survival_days\"] = combined[\"survival_days\"].clip(lower=0)\n",
    "    combined[\"surv_days_w_mean_imps\"] = combined[\"surv_days_w_mean_imps\"].clip(lower=0)\n",
    "\n",
    "    # Create binary outcome\n",
    "    combined[\"survived_90\"] = combined[\"survival_days\"] > 90\n",
    "\n",
    "    # Do one-hot encoding across the entire dataset\n",
    "    X_all = pd.get_dummies(combined.drop(columns=[\"subject_id\", \"survival_days\", \"survived_90\", \"event\", \"surv_days_w_mean_imps\", \"delta\"]))\n",
    "    y_all = combined[\"surv_days_w_mean_imps\"]\n",
    "    y_event = combined[\"event\"]\n",
    "\n",
    "    # Rebuild structured target\n",
    "    y_struct_all = Surv.from_arrays(event=y_event.astype(bool), time=y_all)\n",
    "\n",
    "    # Re-split into original pieces\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "    train_cumsum = [0] + list(pd.Series(train_lengths).cumsum())\n",
    "    test_cumsum = [train_cumsum[-1]] + list((pd.Series(test_lengths).cumsum() + train_cumsum[-1]))\n",
    "\n",
    "    # Slice train\n",
    "    for i in range(len(train_dfs)):\n",
    "        X_train.append(X_all.iloc[train_cumsum[i]:train_cumsum[i+1]].reset_index(drop=True))\n",
    "        y_train.append(y_struct_all[train_cumsum[i]:train_cumsum[i+1]])\n",
    "\n",
    "    # Slice test\n",
    "    for i in range(len(test_dfs)):\n",
    "        X_test.append(X_all.iloc[test_cumsum[i]:test_cumsum[i+1]].reset_index(drop=True))\n",
    "        y_test.append(y_struct_all[test_cumsum[i]:test_cumsum[i+1]])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = splitXY(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dae2feb4-9187-499a-b0f3-b66c4e9c2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rsf(X_train_set, y_train_set, X_test_set, y_test_set, plot_fp=False):\n",
    "    rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10,\n",
    "                               min_samples_leaf=15, random_state=7)\n",
    "    rsf.fit(X_train_set, y_train_set)\n",
    "\n",
    "    # Predict survival functions\n",
    "    surv_funcs = rsf.predict_survival_function(X_test_set)\n",
    "\n",
    "    # Define evaluation time points\n",
    "    time_buckets = np.array([7, 30, 60, 180, 365, 730])\n",
    "    surv_probs = np.row_stack([fn(time_buckets) for fn in surv_funcs])\n",
    "    risk_scores = 1 - surv_probs  # Higher risk = lower survival\n",
    "\n",
    "    c_rsf = []\n",
    "    \n",
    "    print(\"  RSF C-index per time bucket:\")\n",
    "    for i, t in enumerate(time_buckets):\n",
    "        c_index, _, _, _, _ = concordance_index_censored(\n",
    "            y_test_set[\"event\"], [x[1] for x in y_test_set], risk_scores[:, i]\n",
    "        )\n",
    "        print(f\"    Day {t:4d}: C-index = {c_index:.4f}\")\n",
    "        c_rsf.append(c_index)\n",
    "    print(f\"  Mean RSF C-index: {sum(c_rsf) / len(c_rsf):4f}\")\n",
    "\n",
    "    # Full IBS over a grid\n",
    "    surv_probs_grid = np.row_stack([fn(time_buckets) for fn in surv_funcs])\n",
    "    ibs = integrated_brier_score(y_train_set, y_test_set, surv_probs_grid, time_buckets)\n",
    "    print(f\"  RSF IBS:     {ibs:.4f}\")\n",
    "\n",
    "    # Permutation importance\n",
    "    if plot_fp:\n",
    "        perm_result = permutation_importance(\n",
    "            rsf, X_test_set, y_test_set, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        sorted_idx = perm_result.importances_mean.argsort()[::-1][:20]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(sorted_idx)), perm_result.importances_mean[sorted_idx][::-1], align='center')\n",
    "        plt.yticks(range(len(sorted_idx)), X_test_set.columns[sorted_idx][::-1])\n",
    "        plt.xlabel(\"Permutation Importance (Mean decrease in score)\")\n",
    "        plt.title(\"RSF Permutation Feature Importance (Top 20)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def run_gb(X_train_set, y_train_set, X_test_set, y_test_set, plot_fp=False):\n",
    "    gb = GradientBoostingSurvivalAnalysis(n_estimators=100, random_state=7)\n",
    "    gb.fit(X_train_set, y_train_set)\n",
    "\n",
    "    # Predict survival functions\n",
    "    surv_funcs = gb.predict_survival_function(X_test_set)\n",
    "\n",
    "    # Define time buckets\n",
    "    time_buckets = np.array([7, 30, 60, 180, 365, 730])\n",
    "    surv_probs = np.row_stack([fn(time_buckets) for fn in surv_funcs])\n",
    "    risk_scores = 1 - surv_probs  # Higher risk = lower survival\n",
    "\n",
    "    c_gb = []\n",
    "    \n",
    "    print(\"  GB C-index per time bucket:\")\n",
    "    for i, t in enumerate(time_buckets):\n",
    "        c_index, _, _, _, _ = concordance_index_censored(\n",
    "            y_test_set[\"event\"], [x[1] for x in y_test_set], risk_scores[:, i]\n",
    "        )\n",
    "        print(f\"    Day {t:4d}: C-index = {c_index:.4f}\")\n",
    "        c_gb.append(c_index)\n",
    "    print(f\"  Mean GB C-index: {sum(c_gb) / len(c_gb):4f}\")\n",
    "    \n",
    "    # IBS\n",
    "    surv_probs_grid = np.row_stack([fn(time_buckets) for fn in surv_funcs])\n",
    "    ibs = integrated_brier_score(y_train_set, y_test_set, surv_probs_grid, time_buckets)\n",
    "    print(f\"  GB IBS:      {ibs:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    if plot_fp:\n",
    "        fi_gb = gb.feature_importances_\n",
    "        top_idx = np.argsort(fi_gb)[::-1][:20]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top_idx)), fi_gb[top_idx][::-1], align='center')\n",
    "        plt.yticks(range(len(top_idx)), X_test_set.columns[top_idx][::-1])\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.title(\"Gradient Boosting - Top 20 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "321c0519-013f-4669-942f-a4d5f5ea2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set 0, Test Set 3:\n",
      "  RSF C-index per time bucket:\n",
      "    Day    7: C-index = 0.7930\n",
      "    Day   30: C-index = 0.7882\n",
      "    Day   60: C-index = 0.8027\n",
      "    Day  180: C-index = 0.8296\n",
      "    Day  365: C-index = 0.8305\n",
      "    Day  730: C-index = 0.8321\n",
      "  Mean RSF C-index: 0.812691\n",
      "  RSF IBS:     0.0800\n",
      "\n",
      "  GB C-index per time bucket:\n",
      "    Day    7: C-index = 0.8016\n",
      "    Day   30: C-index = 0.8016\n",
      "    Day   60: C-index = 0.8016\n",
      "    Day  180: C-index = 0.8016\n",
      "    Day  365: C-index = 0.8016\n",
      "    Day  730: C-index = 0.8016\n",
      "  Mean GB C-index: 0.801637\n",
      "  GB IBS:      0.0773\n",
      "\n",
      "\n",
      "Train Set 1, Test Set 4:\n",
      "  RSF C-index per time bucket:\n",
      "    Day    7: C-index = 0.7848\n",
      "    Day   30: C-index = 0.7843\n",
      "    Day   60: C-index = 0.7990\n",
      "    Day  180: C-index = 0.8284\n",
      "    Day  365: C-index = 0.8288\n",
      "    Day  730: C-index = 0.8381\n",
      "  Mean RSF C-index: 0.810561\n",
      "  RSF IBS:     0.0800\n",
      "\n",
      "  GB C-index per time bucket:\n",
      "    Day    7: C-index = 0.7994\n",
      "    Day   30: C-index = 0.7994\n",
      "    Day   60: C-index = 0.7994\n",
      "    Day  180: C-index = 0.7994\n",
      "    Day  365: C-index = 0.7994\n",
      "    Day  730: C-index = 0.7994\n",
      "  Mean GB C-index: 0.799400\n",
      "  GB IBS:      0.0797\n",
      "\n",
      "\n",
      "Train Set 2, Test Set 2:\n",
      "  RSF C-index per time bucket:\n",
      "    Day    7: C-index = 0.7881\n",
      "    Day   30: C-index = 0.7846\n",
      "    Day   60: C-index = 0.7974\n",
      "    Day  180: C-index = 0.8247\n",
      "    Day  365: C-index = 0.8263\n",
      "    Day  730: C-index = 0.8307\n",
      "  Mean RSF C-index: 0.808631\n",
      "  RSF IBS:     0.0805\n",
      "\n",
      "  GB C-index per time bucket:\n",
      "    Day    7: C-index = 0.8077\n",
      "    Day   30: C-index = 0.8077\n",
      "    Day   60: C-index = 0.8077\n",
      "    Day  180: C-index = 0.8077\n",
      "    Day  365: C-index = 0.8077\n",
      "    Day  730: C-index = 0.8077\n",
      "  Mean GB C-index: 0.807695\n",
      "  GB IBS:      0.0757\n",
      "\n",
      "\n",
      "Train Set 3, Test Set 0:\n",
      "  RSF C-index per time bucket:\n",
      "    Day    7: C-index = 0.7859\n",
      "    Day   30: C-index = 0.7841\n",
      "    Day   60: C-index = 0.7990\n",
      "    Day  180: C-index = 0.8256\n",
      "    Day  365: C-index = 0.8269\n",
      "    Day  730: C-index = 0.8287\n",
      "  Mean RSF C-index: 0.808374\n",
      "  RSF IBS:     0.0801\n",
      "\n",
      "  GB C-index per time bucket:\n",
      "    Day    7: C-index = 0.7995\n",
      "    Day   30: C-index = 0.7995\n",
      "    Day   60: C-index = 0.7995\n",
      "    Day  180: C-index = 0.7995\n",
      "    Day  365: C-index = 0.7995\n",
      "    Day  730: C-index = 0.7995\n",
      "  Mean GB C-index: 0.799516\n",
      "  GB IBS:      0.0793\n",
      "\n",
      "\n",
      "Train Set 4, Test Set 1:\n",
      "  RSF C-index per time bucket:\n",
      "    Day    7: C-index = 0.7744\n",
      "    Day   30: C-index = 0.7690\n",
      "    Day   60: C-index = 0.7838\n",
      "    Day  180: C-index = 0.8150\n",
      "    Day  365: C-index = 0.8168\n",
      "    Day  730: C-index = 0.8204\n",
      "  Mean RSF C-index: 0.796568\n",
      "  RSF IBS:     0.0806\n",
      "\n",
      "  GB C-index per time bucket:\n",
      "    Day    7: C-index = 0.8049\n",
      "    Day   30: C-index = 0.8049\n",
      "    Day   60: C-index = 0.8049\n",
      "    Day  180: C-index = 0.8049\n",
      "    Day  365: C-index = 0.8049\n",
      "    Day  730: C-index = 0.8049\n",
      "  Mean GB C-index: 0.804885\n",
      "  GB IBS:      0.0807\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices = [0, 1, 2, 3, 4]\n",
    "random.shuffle(indices)\n",
    "\n",
    "for i, j in enumerate(indices):\n",
    "    print(f\"Train Set {i}, Test Set {j}:\")\n",
    "    run_rsf(X_train[i], y_train[i], X_test[j], y_test[j])\n",
    "    print()\n",
    "    run_gb(X_train[i], y_train[i], X_test[j], y_test[j])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2df6a1-4dc6-4133-b90c-b77142123ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
